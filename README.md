# mpi-test

##Настройки для macOS:

`brew install openmpi`

##MPI:

`MPI_Send(void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm)`
IN buf	-	адрес начала расположения пересылаемых данных;
IN count	-	число пересылаемых элементов;
IN datatype	-	тип посылаемых элементов;
IN dest	-	номер процесса-получателя в группе, связанной с коммуникатором comm;
IN tag	-	идентификатор сообщения (аналог типа сообщения функций nread и nwrite PSE nCUBE2);
IN comm	-	коммуникатор области связи.


`int MPI_Recv(void* buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status *solvingStatus)`
OUT	buf	-	адрес начала расположения принимаемого сообщения;
IN	count	-	максимальное число принимаемых элементов;
IN	datatype	-	тип элементов принимаемого сообщения;
IN	source	-	номер процесса-отправителя;
IN	tag	-	идентификатор сообщения;
IN	comm	-	коммуникатор области связи;
OUT	solvingStatus	-	атрибуты принятого сообщения.


`int MPI_Barrier(MPI_Comm comm)`
Функция барьерной синхронизации MPI_BARRIER блокирует вызывающий процесс, пока все процессы группы не вызовут её.
В каждом процессе управление возвращается только тогда, когда все процессы в группе вызовут процедуру.

Широковещательная рассылка данных выполняется с помощью функции MPI_Bcast. Процесс с номером root рассылает сообщение
из своего буфера передачи всем процессам области связи коммуникатора comm.

Математические операции над блоками данных, распределенных по процессорам, называют глобальными операциями редукции.
В общем случае операцией редукции называется операция, аргументом которой является вектор,
а результатом - скалярная величина, полученная применением некоторой математической операции ко всем компонентам вектора

В MPI глобальные операции редукции представлены в нескольких вариантах:
с сохранением результата в адресном пространстве одного процесса (MPI_Reduce).
с сохранением результата в адресном пространстве всех процессов (MPI_Allreduce).


Семейство функций распределения блоков данных по всем процессам группы состоит из двух подпрограмм: MPI_Scatter и MPI_Scaterv.

Функция MPI_Scatter разбивает сообщение из буфера посылки процесса root на равные части размером sendcount и посылает i-ю часть в буфер приема процесса с номером i (в том числе и самому себе). Процесс root использует оба буфера (посылки и приема), поэтому в вызываемой им подпрограмме все параметры являются существенными. Остальные процессы группы с коммуникатором comm являются только получателями, поэтому для них параметры, специфицирующие буфер посылки, не существенны.

IN	sendbuf	-	адрес начала размещения блоков распределяемых данных (используется только в процессе-отправителе root);
IN	sendcount	-	число элементов, посылаемых каждому процессу;
IN	sendtype	-	тип посылаемых элементов;
OUT	recvbuf	-	адрес начала буфера приема;
IN	recvcount	-	число получаемых элементов;
IN	recvtype	-	тип получаемых элементов;
IN	root	-	номер процесса-отправителя;
IN	comm	-	коммуникатор.

